---
title: 'PS3: Scrape and plot data'
author: "Pooja Sadarangani"
date: "2022-10-28"
output:
  pdf_document: default
  html_document: default
collaborators: Pranali Oza
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, warning = FALSE)
```

# 1.1 Ethical issues


## 1. Consult wikipedia terms of usage. Does it put restrictions on web scraping?


### There are no explicit restrictions on web scraping in wikipedia's terms of usage. However the scrapers must keep the follwing points in mind while scraping wiki pages:
### "Disrupting the services by placing an undue burden on a Project website or the networks or servers connected with a Project website;"
### "Disrupting the services by inundating any of the Project websites with communications or other traffic that suggests no serious intent to use the Project website for its stated purpose;"


## 2. Consult robots.txt. Is it permitted to scrape wiki-pages?

```{r}
library(rvest)
library(robotstxt)
robotstxtfile <- read_html("https://en.wikipedia.org/robots.txt")
paths_allowed(path = "https://en.wikipedia.org/robots.txt")
```

### According to the robots.txt, we have the permission to scrape wiki-pages but we are expected to to do it responsibly.


## 3.  Describe what do you do in order to reduce the burden to wikipedia website. 


### I can do the followings things to reduce the burden to wikipedia website:
### 1. Download the webpages once first, and download more only if required
### 2. Use cached version for developing and deploying
### 3. Limit query requests to what the server can handle
### 4. Do not download the pages that we are not allowed to scrape after consulting to robots.txt
### 5. Donate money to wikipedia


# 1.2 Parse the list of mountains


## 1. Load the wikipedia list of mountains by height 


```{r}
library(rvest)
page <- read_html("/Users/poojasadarangani/Pooja/UW College Work/IMT 573/List of mountains by elevation - Wikipedia.html")
```


## 2. Find all the tables there in the html.


```{r}
count <- page %>%
   html_elements("table") %>%
   html_table()
length(count)
```


## 3. Find the table headers, and determine which columns are mountain names, heights, and where are the links to the individual mountain pages.


```{r}
library(dplyr)
library(tidyverse)
# Extracting table headers
page %>%
   html_elements("th") %>%
   html_text()

# Extracting table for determining which column number corresponds to which column name
table_structure <- page %>%
  html_elements (xpath = "//table") %>%
  html_table(fill = TRUE)
table_structure[1]

#page %>% 
  #html_elements("td") %>%
  #html_elements("a") %>%
  #html_attr("href") 
```


### From the above table structure, we can tell that montain names are in column 1 and height is in column 2. By looking at the table structure, we are unable to tell where the links are stored.  However,from the html_code we can tell that the links are present in the td element -> a element -> attribute href


## 4. Create a data frame that contains names and heights of the mountains above 6800m, and the links to the corresponding wikipedia pages. You’ll add longitude and latitude for each mountain in this data frame later.



```{r}
library(dplyr)
library(tidyverse)
# Extracting mountain names
Mountain_name <- page %>%
  html_elements(xpath = '//table//td[1]') %>% 
  html_text()
#length(Mountain_name)

# Extracting heights of the mountains
Height <- page %>%
  html_elements(xpath = '//table//td[2]') %>% 
  html_text()
#length(Height)
Heights <- gsub(",","", Height)
Heights <- as.numeric(Heights)

# Extracting URLs linked with the mountains
Links <- page %>%
  html_elements(xpath = '//table//td[1]//a[1]//@href') %>%
  html_text()
#length(Links)

# Creating a dataframe
mountain_dataframe <- data.frame()
temp_dataframe <- cbind(Mountain_name, Heights, Links)
mountain_dataframe <- rbind(mountain_dataframe, temp_dataframe)
# Converting height column from character to numeric for filtering
mountain_dataframe$Heights = as.numeric(mountain_dataframe$Heights)

filtered_data <- filter(mountain_dataframe, Heights > 6800)

cat("The number of rows after filtering mountains having height greater than 6800m", nrow(filtered_data))
```


## 5. Print a small sample of your data frame to see that it looks reasonable.


```{r}
head(filtered_data)
```


# 1.3 Scrape the individual mountain data


## 1. Write a function that converts the longitude/latitude string to degrees (positive and negative)


## 2. Write another function that takes link as an argument and loads the mountain’s html page and extracts latitude and longitude.


```{r}
library(rvest)
library(tidyverse)
library(dplyr)
# Function to convert longitude/latitude string to degrees
convert_function <- function(string){
direction <- if(grepl("[WS]", string)) -1 else 1
dms <- strsplit(string,"°|′|″")
dd <- as.numeric(dms[[1]][1])
mm <- as.numeric(dms[[1]][2])
ss <- as.numeric(dms[[1]][3])
return ((dd + mm/60 + ss/3600)*direction)
}

# Function to read file and extract latitude and longitude
scrape_function <- function(url){
page1 <- try(read_html(url), silent = TRUE)
if (inherits(page1, "try-error")){
return(NULL)
}
  
  lat <- page1 %>%
  html_element("span.latitude") %>%
  html_text()

  long <- page1 %>%
 html_element("span.longitude") %>%
  html_text()
  
  return(c(lat,long))
}
```


## 3. loop over the table of mountains you did above, download the mountain data, and extract the coordinates. Store these into the same data frame.


```{r}
library(rvest)
library(tidyverse)
library(dplyr)
lat_df <-NULL
long_df <- NULL
for(url in filtered_data$Links){
page1 <- scrape_function(url) #Loading URL 
#cat("debug", page1)
  #print("hi")
  #lat <- page1 %>%
   #html_element("span.latitude") %>%
   #html_text()

#long <- page1 %>%
 #  html_element("span.longitude") %>%
  # html_text()
  
  lat = page1[1] # Extracting latitude in URL
  long = page1[2] # Extracting longitude in URL
converted_lat <- convert_function(lat) # Converting latitude string to degrees
converted_long <- convert_function(long) # Converting longitude string to degrees

lat_df <- rbind(lat_df,converted_lat) 
long_df <- rbind(long_df, converted_long)

#else{
 # Links <- NA
  #converted_lat <- NA
  #converted_long <- NA
  
  #filtered_data <- filtered_data %>%
  #mutate(latitude = NA)

#filtered_data <- filtered_data %>%
  #mutate(longitude = NA)

#}
}

filtered_data <- filtered_data %>%
  mutate(latitude = lat_df) # Appending column latitude to df

filtered_data <- filtered_data %>%
  mutate(longitude = long_df) # # Appending column longitude to df

filtered_data <- filtered_data[!is.na(filtered_data$latitude),] # Removing NA values
filtered_data <- filtered_data[!is.na(filtered_data$longitude),] # Removing NA values

names(filtered_data)

```


## Print a sample of the dataframe and check that it looks good. How many mountains did you get?

```{r}
head(filtered_data)
cat("The number of mountains that I got are after the removal of mountains having invalid links: ", nrow(filtered_data))
```


# 1.4 Plot the mountains


## 1. Plot all the mountains on a world map. Color those according to their height.


```{r}
library(maps)
library(ggplot2)
world <- map_data("world")
ggplot(world) +
  geom_polygon(aes(long, lat, group=group),
               col="white", fill="gray") +
  geom_point(data=filtered_data, aes(longitude, latitude, color = filtered_data$Heights))+
  coord_quickmap()
```


## 2. Describe what did you get. Where are the tall mountains located? Do all the locations make sense (i.e. you do not have mountains in the middle of sea and such)?


### From looking at the graph, I can tell that there are 2 mountains situated in South Amaerica, rest all tall mountains are in Asia. All locations make sense as they all seem to be on land and not in sea.